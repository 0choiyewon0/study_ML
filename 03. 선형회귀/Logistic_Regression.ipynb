{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 선형 회귀\n",
    "\n",
    "\n",
    "\n",
    "1. 다중 선형 회귀(Multiple Linear Regression)\n",
    "    - X(수치형), Y(연속형)의 관계를 선형으로 가정하여 잘 표현할 수 있는 회귀계수를 데이터로부터 추정하는 모델\n",
    "    - 회귀 계수 결정법 (Direct Solution)\n",
    "        * 실제값과 모델 예측값의차이를 통해 오차제곱합을 최소로 하는 값을 회귀 계수로 선정 \n",
    "        * 미분값 0\n",
    "    - 회귀 계수 결정법 (Numerical Search)\n",
    "        * 경사하강법(gradient descent) 와 같은 반복적인 방식으로 선형회귀 계수를 구할 수 있음\n",
    "        * 경사하강법 : 어떤 함수 값(최소화를 원하는 값)을 최소화 하기 위해 임의읭 시작점을 잡은 후 해당 지점에서의 그래디언트를 구하고, 이를 반대 방향으로 조금씩 이동하는 과정을 여러번 반복\n",
    "        * 종류 : \n",
    "            - GD(Batch Gradient Descent)\n",
    "            : 파라미터를 업데이트 할 때마다 모든 학습 데이터를 사용하여 비용 함수의 그레디언트를 계산 , 매우 낮은 학습 효율을 보일 수 있음\n",
    "            - SGD(Stochastic Gradient Descent)\n",
    "            : 파라미터를 업데이트 할 때, 무작위로 샘플링된 학습 데이터를 하나씨만 이용하여 비용함수의 그레디언트 계산\n",
    "              , 모델을 자주 업데이트 하며, 성능 개선 정도를 빠르게 확인 가능\n",
    "              , Local minima에 빠질 가능성을 줄임\n",
    "              , 최소 비용에 수렴했는지의 판단이 상대적으로 어려움\n",
    "            - Mini Batch Cradient Descent\n",
    "            : 파라미터를 업데이트 할 때마다 일정량의 일부 데이터를 무작위로 뽑아 비용함수의 그레디언트를 계산\n",
    "            , 앞선 두가지의 개념을 혼합\n",
    "            , 널리 사용되는 기법\n",
    "   \n",
    "2. 정규화(reqularization)\n",
    "    - 회귀계수가 가질 수 있는 값에 제약조건을 부여하여 미래 데이터에 대한 오차 기대\n",
    "    - variance(분산)와 bias를 통하여 오차의 기댓값 측정 -> variance를 감소시켜 일반화 성능을 높이는 기법을 정규화 과정임\n",
    "    \n",
    "    \n",
    "    - Bias : 과녁에 떨어진 정도 \n",
    "    - Variance : 서로 떨어진 정도\n",
    "    \n",
    "3. Bias-Variance Decomposition\n",
    "    - 일반화 성능을 높이는 정규화, 앙상블 기법의 이론적 배경\n",
    "    - 학습에 쓰지 않는 미래데이터에 대한 오차의 기대값을 모델의 Bias, Variance로 분해하자는 내용\n",
    "    - Boosting : Bias 줄여 성능 높임\n",
    "    - Lasso : Variance 줄여 성능 높임\n",
    "    \n",
    "    - Bias(Low), Variance(High) : NN, SVM, kNN\n",
    "    - Bias(High), Variance(Low) : 로지스틱회귀, LDA, KNN\n",
    "    \n",
    "4. 특징\n",
    "    - 분류 문제는 Linear regression으로 풀 수 없음\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 로지스틱 함수(Logistic function)\n",
    "\n",
    "1. 정의 : 출력 결과가 항상 0~1 사이의 값이 되는 함수\n",
    "2. 승산(Odds) : 임의의 사건 A가 발생하지 않을 확률 대비 일어날 확률의 비율\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "71idpMwjOO4A",
    "outputId": "23bf5636-a144-44ab-86bd-744c5397d635"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris') \n",
    "\n",
    "X = iris.drop('species', axis=1) \n",
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vtp2SzueOO4Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder   \n",
    "classle = LabelEncoder()\n",
    "y = classle.fit_transform(iris['species'].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iYQQ8kFPOO4e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yLX_YqrfOO4n"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 표준화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 로지스틱 회귀 실습\n",
    "\n",
    "1. C값이 클수록 규제화의 강도가 줆\n",
    "2. 파라미터 구성\n",
    "    설정 값을 주지 않으면 default 값으로 진행\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DqMtIchFOO4w"
   },
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# C = 1/λ. 디폴트: L2, Auto.  \n",
    "Logit = LogisticRegression(C=200, random_state=11)  \n",
    "l_1=Logit.fit(X_train_std, y_train) \n",
    "y_train_pred = Logit.predict(X_train_std)\n",
    "y_test_pred = Logit.predict(X_test_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 로지스틱 회귀 평가\n",
    "\n",
    "1. 정밀도(accuracy), 실험 데이터 정밀도, 정확도 행렬(confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "dUZlivz8OO4-",
    "outputId": "27371bb4-4b97-4fd1-9041-ff8b389c10bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9809523809523809\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train,y_train_pred))  \n",
    "print(accuracy_score(y_test,y_test_pred))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "sb6DOXFxOO5G",
    "outputId": "3793ad46-83ad-4607-9a48-63929dd9d712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_test_pred))  # Confusion matrix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML 04장_Logistic Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
